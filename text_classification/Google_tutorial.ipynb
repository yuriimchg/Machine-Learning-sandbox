{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Studying text classification\n",
    "\n",
    "Build a simple text classification model with this tutorial: \n",
    "\n",
    "https://developers.google.com/machine-learning/guides/text-classification/step-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sample, path_to_data='aclImdb'):\n",
    "    \n",
    "    texts, labels = [], []\n",
    "    for category in ['pos', 'neg']:\n",
    "        dir_path = os.path.join(path_to_data, sample, category)\n",
    "        # Read all data from \".txt\" files\n",
    "        for filename in sorted(os.listdir(dir_path)):\n",
    "            if filename.endswith('.txt'):\n",
    "                # Add features\n",
    "                with open(os.path.join(dir_path, filename), 'r') as f:\n",
    "                    texts.append(f.read())\n",
    "                # Add labels\n",
    "                labels.append(0 if category == 'neg' else 1)\n",
    "                \n",
    "    return texts, labels\n",
    "    \n",
    "\n",
    "\n",
    "def load_imdb_sentiment_analysis_data(random_seed=42):\n",
    "    abs_path = os.path.abspath('aclImdb')\n",
    "    \n",
    "    # Training_data\n",
    "    train_texts, train_labels = load_data(sample='train')\n",
    "    test_texts, test_labels = load_data(sample='test')\n",
    "    \n",
    "    # Now shuffle the training data only\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(train_texts)\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(train_labels)\n",
    "    \n",
    "    return (train_texts, np.array(train_labels), test_texts, np.array(test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how does data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_texts, train_labels, test_texts, test_labels)  = load_imdb_sentiment_analysis_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This movie seemed like it was going to be better than it ended up being. The cinematography is good, the acting seemed solid, the dialogue wasn\\'t too stiff... but then about twenty minutes in there\\'s this long scene with a Doctor who you know is actually a patient at the asylum pretending to be a Doctor - and it just goes south from there.<br /><br />On top of that, the demon is about the silliest looking hellspawn since the Godzilla-looking thing in Curse of the Demon. There\\'s also some odd demon worshippers who wear masks that look like the exploding teens from the beginning of Logan\\'s Run.<br /><br />In the end, the cinematography couldn\\'t save this movie. Despite some pretty solid performances by the actors, the story just doesn\\'t go anywhere. I think \"Hellbored\" would have been a better title for this.',\n",
       " 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[99], train_labels[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_per_sample(sample_texts):\n",
    "    num_words = [len(s.split(' ')) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.hist([len(s.split(' ')) for s in sample_texts], bins=100, color='red')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.xlabel('Lenght of sample')\n",
    "    plt.ylabel('Count of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def get_num_classes(labels):\n",
    "    \n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm for Data Preparation and Model Building\n",
    "1. Calculate the number of samples/number of words per sample ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a\n",
    "simple multi-layer perceptron (MLP) model to classify them (left branch in the\n",
    "flowchart below):\n",
    "  a. Split the samples into word n-grams; convert the n-grams into vectors.\n",
    "  b. Score the importance of the vectors and then select the top 20K using the scores.\n",
    "  c. Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a\n",
    "   sepCNN model to classify them (right branch in the flowchart below):\n",
    "  a. Split the samples into words; select the top 20K words based on their frequency.\n",
    "  b. Convert the samples into word sequence vectors.\n",
    "  c. If the original number of samples/number of words per sample ratio is less\n",
    "     than 15K, using a fine-tuned pre-trained embedding with the sepCNN\n",
    "     model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find\n",
    "   the best model configuration for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set *vectorization* parameters, limit *number* of features, choose *token mode*, *discard* corpus with frequency < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAM_RANGE = (1, 2)\n",
    "TOP_K = 20000\n",
    "TOKEN_MODE = 'word' # another option is 'char'\n",
    "MIN_DOC_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_ngram(train_texts, train_labels, val_texts):\n",
    "    kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE, # use unigrams & bigrams\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,\n",
    "        'min_df': MIN_DOC_FREQUENCY\n",
    "    }\n",
    "    # Create vectorizer instance\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "    # Learn vocabulary from training set & vectorize training texts\n",
    "    X_train = vectorizer.fit_transform(train_texts)\n",
    "    # transform test set\n",
    "    X_val = vectorizer.transform(val_texts)\n",
    "    \n",
    "    # Select top K vectorized features\n",
    "    selector = SelectKBest(score_func=f_classif, k=min(TOP_K, X_train.shape[1]))\n",
    "    selector.fit(X_train, train_labels)\n",
    "    X_train = selector.transform(X_train).astype('float32')\n",
    "    X_val = selector.transform(X_val).astype('float32')\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_layer(num_classes):\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    op_units, op_activation = get_last_layer(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    \n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "    # Last layer\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    (train_texts, train_labels, val_texts, val_labels) = data\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    \n",
    "    unexpected_labels = [l for l in val_labels if l not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError(f'there are some unexpected labels in validation set such as {unexpected_labels}.')\n",
    "    \n",
    "    # Vectorize texts:\n",
    "    X_train, X_val = vectorize_ngram(train_texts, train_labels, val_texts)\n",
    "    \n",
    "    # Create a multilayer perceptron\n",
    "    model = mlp_model(layers=layers,\n",
    "                      units=units,\n",
    "                      dropout_rate=dropout_rate,\n",
    "                      input_shape=X_train.shape[1:],\n",
    "                      num_classes=num_classes)\n",
    "    # Choose type of loss\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    # Pick an optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "    # Stop training if the loss is not converging\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)]\n",
    "    \n",
    "    history = model.fit(x=X_train,\n",
    "                        y=train_labels,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(X_val, val_labels),\n",
    "                        verbose=2,  # Logs once per epoch.\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('IMDb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1577: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please provide as model inputs either a single array or a list of arrays. You passed: x=  (0, 112)\t0.009362777\n  (0, 253)\t0.01695334\n  (0, 382)\t0.015232822\n  (0, 537)\t0.011793215\n  (0, 680)\t0.025807288\n  (0, 708)\t0.03491824\n  (0, 752)\t0.028357456\n  (0, 769)\t0.046636384\n  (0, 958)\t0.02702803\n  (0, 964)\t0.019627532\n  (0, 972)\t0.01821091\n  (0, 1182)\t0.03337962\n  (0, 1215)\t0.019942911\n  (0, 1377)\t0.0247151\n  (0, 1402)\t0.0376643\n  (0, 1620)\t0.01566047\n  (0, 1647)\t0.021632886\n  (0, 1775)\t0.0083106775\n  (0, 2094)\t0.02946515\n  (0, 2132)\t0.031468213\n  (0, 2137)\t0.026044557\n  (0, 2259)\t0.023916056\n  (0, 2271)\t0.019564958\n  (0, 2281)\t0.011643428\n  (0, 2308)\t0.035241667\n  :\t:\n  (24999, 12125)\t0.04255081\n  (24999, 12777)\t0.07368688\n  (24999, 13663)\t0.069687724\n  (24999, 14262)\t0.046546824\n  (24999, 14265)\t0.0627084\n  (24999, 14437)\t0.030904347\n  (24999, 14472)\t0.045807716\n  (24999, 15214)\t0.04087393\n  (24999, 15591)\t0.06357261\n  (24999, 16119)\t0.032770094\n  (24999, 16141)\t0.059285223\n  (24999, 16152)\t0.08767459\n  (24999, 16180)\t0.09161612\n  (24999, 16244)\t0.091852345\n  (24999, 16510)\t0.086049095\n  (24999, 17213)\t0.0159686\n  (24999, 17553)\t0.0154418275\n  (24999, 17705)\t0.03997247\n  (24999, 17808)\t0.03593197\n  (24999, 17809)\t0.07382394\n  (24999, 18393)\t0.041293267\n  (24999, 18395)\t0.07244558\n  (24999, 18699)\t0.020861581\n  (24999, 18877)\t0.05168561\n  (24999, 19374)\t0.034672137",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-068f15c68c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ngram_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_imdb_sentiment_analysis_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-f33266bd2bf7>\u001b[0m in \u001b[0;36mtrain_ngram_model\u001b[0;34m(data, learning_rate, epochs, batch_size, layers, units, dropout_rate)\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Logs once per epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                         batch_size=batch_size)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_envs/nlp/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1536\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_envs/nlp/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_element\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     x, y, sample_weights = self._standardize_weights(x, y, sample_weight,\n\u001b[0;32m--> 992\u001b[0;31m                                                      class_weight, batch_size)\n\u001b[0m\u001b[1;32m    993\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jupyter_envs/nlp/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_weights\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m           raise ValueError('Please provide as model inputs either a single '\n\u001b[0;32m-> 1024\u001b[0;31m                            'array or a list of arrays. You passed: x=' + str(x))\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0mall_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Please provide as model inputs either a single array or a list of arrays. You passed: x=  (0, 112)\t0.009362777\n  (0, 253)\t0.01695334\n  (0, 382)\t0.015232822\n  (0, 537)\t0.011793215\n  (0, 680)\t0.025807288\n  (0, 708)\t0.03491824\n  (0, 752)\t0.028357456\n  (0, 769)\t0.046636384\n  (0, 958)\t0.02702803\n  (0, 964)\t0.019627532\n  (0, 972)\t0.01821091\n  (0, 1182)\t0.03337962\n  (0, 1215)\t0.019942911\n  (0, 1377)\t0.0247151\n  (0, 1402)\t0.0376643\n  (0, 1620)\t0.01566047\n  (0, 1647)\t0.021632886\n  (0, 1775)\t0.0083106775\n  (0, 2094)\t0.02946515\n  (0, 2132)\t0.031468213\n  (0, 2137)\t0.026044557\n  (0, 2259)\t0.023916056\n  (0, 2271)\t0.019564958\n  (0, 2281)\t0.011643428\n  (0, 2308)\t0.035241667\n  :\t:\n  (24999, 12125)\t0.04255081\n  (24999, 12777)\t0.07368688\n  (24999, 13663)\t0.069687724\n  (24999, 14262)\t0.046546824\n  (24999, 14265)\t0.0627084\n  (24999, 14437)\t0.030904347\n  (24999, 14472)\t0.045807716\n  (24999, 15214)\t0.04087393\n  (24999, 15591)\t0.06357261\n  (24999, 16119)\t0.032770094\n  (24999, 16141)\t0.059285223\n  (24999, 16152)\t0.08767459\n  (24999, 16180)\t0.09161612\n  (24999, 16244)\t0.091852345\n  (24999, 16510)\t0.086049095\n  (24999, 17213)\t0.0159686\n  (24999, 17553)\t0.0154418275\n  (24999, 17705)\t0.03997247\n  (24999, 17808)\t0.03593197\n  (24999, 17809)\t0.07382394\n  (24999, 18393)\t0.041293267\n  (24999, 18395)\t0.07244558\n  (24999, 18699)\t0.020861581\n  (24999, 18877)\t0.05168561\n  (24999, 19374)\t0.034672137"
     ]
    }
   ],
   "source": [
    "train_ngram_model(data=load_imdb_sentiment_analysis_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_imdb_sentiment_analysis_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1577: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = vectorize_ngram(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
