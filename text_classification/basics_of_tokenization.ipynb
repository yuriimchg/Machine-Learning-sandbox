{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yurii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### basic usage of regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very|nice|lecture|day|moon'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['very','nice','lecture','day','moon']\n",
    "expression = '|'.join(words)\n",
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very', 'nice', 'lecture']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(expression, 'i attended a very nice lecture last year', re.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'been',\n",
       " 'for',\n",
       " 'has',\n",
       " 'john',\n",
       " 'phase',\n",
       " 'selected',\n",
       " 'the',\n",
       " 'this',\n",
       " 'time',\n",
       " 'trial'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'John has been selected for the trial phase this time. Congrats!!'\n",
    "sentence=sentence.lower()\n",
    "sentence=sentence.replace('!','').replace('.', '')\n",
    "words= sentence.split(' ')\n",
    "\n",
    "# Define positive & negative words\n",
    "positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful','congrats']\n",
    "negative_words=['awful','lame','horrible','bad']\n",
    "\n",
    "set(words)-set(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with stop-words: ['john', 'has', 'been', 'selected', 'for', 'the', 'trial', 'phase', 'this', 'time', 'congrats']\n",
      "Sentence without stop-words: ['john', 'selected', 'trial', 'phase', 'time', 'congrats']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "print(f'Sentence with stop-words: {tokens}')\n",
    "print(f'Sentence without stop-words: {new_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'has',\n",
       " 'been',\n",
       " 'selected',\n",
       " 'for',\n",
       " 'the',\n",
       " 'trial',\n",
       " 'phase',\n",
       " 'this',\n",
       " 'time',\n",
       " 'congrats']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, set)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words), type(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many times a specific word occurs in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john has been selected for the trial phase this time congrats\n",
      "[0 0 0 0 1 0 0 0 0 0 0] been\n",
      "[0 0 0 1 0 0 0 0 0 0 0] congrats\n",
      "[1 0 0 0 0 0 0 0 0 0 0] for\n",
      "[0 0 0 0 0 0 1 0 0 0 0] has\n",
      "[0 0 1 0 0 0 0 0 0 0 0] john\n",
      "[0 0 0 0 0 0 0 1 0 0 0] phase\n",
      "[0 0 0 0 0 0 0 0 0 0 1] selected\n",
      "[0 0 0 0 0 1 0 0 0 0 0] the\n",
      "[0 0 0 0 0 0 0 0 1 0 0] this\n",
      "[0 0 0 0 0 0 0 0 0 1 0] time\n",
      "[0 1 0 0 0 0 0 0 0 0 0] trial\n",
      "\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0] as\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0] been\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] by\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] charge\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0] described\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] environment\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] experiences\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] filmed\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] have\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0] her\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0] his\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] in\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0] instead\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] officer\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] on\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0] ordeal\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0] phone\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] provided\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] recall\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] safe\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] she\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] should\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] the\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0] to\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] traumatic\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0] was\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] with\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer_fit = vectorizer.fit_transform(sentence.split()).toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# print()\n",
    "# print(vectorizer_fit.toarray())\n",
    "print(sentence)\n",
    "l = [print(row, feature_names[i]) for i, row in enumerate(vectorizer_fit)]\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "sentence2 = 'She should have been provided with a safe environment to recall her traumatic experiences. Instead she was filmed by the officer in charge on his phone as she described the ordeal.'\n",
    "vectorizer_fit = vectorizer.fit_transform(sentence2.split()).toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "l = [print(vectorizer_fit[i], word) for i, word in enumerate(feature_names)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF  == terms frequency & inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TF = \\frac{\\sum{specWords}}{\\sum{Words}};\n",
    "$$\n",
    "$$\n",
    "IDF = \\log{\\frac{\\sum{docs}}{\\sum{docsWithWord}}}\n",
    "$$\n",
    "Regarding to this:\n",
    "$$\n",
    "TF-IDF = TF * IDF == \\frac{\\sum{specWords}}{\\sum{Words}} \\cdot \\log{\\frac{\\sum{docs}}{\\sum{docsWithWord}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A thorough list of publicly available NLP data sets has already been created by Nicolas Iderhoff', ' Beyond these, here are some projects I can recommend to any NLP novice wanting to get their hands dirty:']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.26255634, 0.        , 0.        , 0.26255634, 0.26255634,\n",
       "         0.        , 0.26255634, 0.        , 0.26255634, 0.26255634,\n",
       "         0.        , 0.        , 0.        , 0.26255634, 0.        ,\n",
       "         0.26255634, 0.26255634, 0.26255634, 0.186811  , 0.        ,\n",
       "         0.26255634, 0.        , 0.26255634, 0.        , 0.26255634,\n",
       "         0.        , 0.        , 0.        , 0.26255634, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.22641916, 0.22641916, 0.        , 0.        ,\n",
       "         0.22641916, 0.        , 0.22641916, 0.        , 0.        ,\n",
       "         0.22641916, 0.22641916, 0.22641916, 0.        , 0.22641916,\n",
       "         0.        , 0.        , 0.        , 0.1610991 , 0.22641916,\n",
       "         0.        , 0.22641916, 0.        , 0.22641916, 0.        ,\n",
       "         0.22641916, 0.22641916, 0.22641916, 0.        , 0.45283832,\n",
       "         0.22641916]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'A thorough list of publicly available NLP data sets has already been created by Nicolas Iderhoff. Beyond these, here are some projects I can recommend to any NLP novice wanting to get their hands dirty:'\n",
    "text = text.split('.')\n",
    "print(text)\n",
    "vector = TfidfVectorizer()\n",
    "X = vector.fit_transform(text)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "data = [\n",
    " ('I love my country.', 'positive'),\n",
    " ('This is an amazing place!', 'positive'),\n",
    " ('I do not like the smell of this place.', 'negative'),\n",
    " ('I do not like this restaurant', 'negative'),\n",
    " ('I am tired of hearing your nonsense.', 'negative'),\n",
    " (\"I always aspire to be like him\", 'positive'),\n",
    " (\"It's a horrible performance.\", \"negative\")\n",
    " ]\n",
    "model = NaiveBayesClassifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('Dont watch it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
