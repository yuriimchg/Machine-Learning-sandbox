{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/yurii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-19 14:26:02--  https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12843 (13K) [application/x-httpd-php]\n",
      "Saving to: ‘blood_data.csv’\n",
      "\n",
      "blood_data.csv      100%[===================>]  12,54K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-04-19 14:26:03 (46,3 MB/s) - ‘blood_data.csv’ saved [12843/12843]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data -O blood_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### basic usage of regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very|nice|lecture|day|moon'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['very','nice','lecture','day','moon']\n",
    "expression = '|'.join(words)\n",
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very', 'nice', 'lecture']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(expression, 'i attended a very nice lecture last year', re.M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'been',\n",
       " 'for',\n",
       " 'has',\n",
       " 'john',\n",
       " 'phase',\n",
       " 'selected',\n",
       " 'the',\n",
       " 'this',\n",
       " 'time',\n",
       " 'trial'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'John has been selected for the trial phase this time. Congrats!!'\n",
    "sentence=sentence.lower()\n",
    "sentence=sentence.replace('!','').replace('.', '')\n",
    "words= sentence.split(' ')\n",
    "\n",
    "# Define positive & negative words\n",
    "positive_words=['awesome','good', 'nice', 'super', 'fun', 'delightful','congrats']\n",
    "negative_words=['awful','lame','horrible','bad']\n",
    "\n",
    "set(words)-set(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with stop-words: ['john', 'has', 'been', 'selected', 'for', 'the', 'trial', 'phase', 'this', 'time', 'congrats']\n",
      "Sentence without stop-words: ['john', 'selected', 'trial', 'phase', 'time', 'congrats']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_tokens = [w for w in tokens if not w in stop_words]\n",
    "\n",
    "print(f'Sentence with stop-words: {tokens}')\n",
    "print(f'Sentence without stop-words: {new_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'has',\n",
       " 'been',\n",
       " 'selected',\n",
       " 'for',\n",
       " 'the',\n",
       " 'trial',\n",
       " 'phase',\n",
       " 'this',\n",
       " 'time',\n",
       " 'congrats']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, set)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words), type(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how many times a specific word occurs in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john has been selected for the trial phase this time congrats\n",
      "[0 0 0 0 1 0 0 0 0 0 0] been\n",
      "[0 0 0 1 0 0 0 0 0 0 0] congrats\n",
      "[1 0 0 0 0 0 0 0 0 0 0] for\n",
      "[0 0 0 0 0 0 1 0 0 0 0] has\n",
      "[0 0 1 0 0 0 0 0 0 0 0] john\n",
      "[0 0 0 0 0 0 0 1 0 0 0] phase\n",
      "[0 0 0 0 0 0 0 0 0 0 1] selected\n",
      "[0 0 0 0 0 1 0 0 0 0 0] the\n",
      "[0 0 0 0 0 0 0 0 1 0 0] this\n",
      "[0 0 0 0 0 0 0 0 0 1 0] time\n",
      "[0 1 0 0 0 0 0 0 0 0 0] trial\n",
      "\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0] as\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0] been\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] by\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] charge\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0] described\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1] environment\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] experiences\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] filmed\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] have\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0] her\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0] his\n",
      "[0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] in\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0] instead\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] officer\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0] on\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0] ordeal\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0] phone\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] provided\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] recall\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] safe\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0] she\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] should\n",
      "[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] the\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0] to\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] traumatic\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0] was\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] with\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer_fit = vectorizer.fit_transform(sentence.split()).toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# print()\n",
    "# print(vectorizer_fit.toarray())\n",
    "print(sentence)\n",
    "l = [print(row, feature_names[i]) for i, row in enumerate(vectorizer_fit)]\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "sentence2 = 'She should have been provided with a safe environment to recall her traumatic experiences. Instead she was filmed by the officer in charge on his phone as she described the ordeal.'\n",
    "vectorizer_fit = vectorizer.fit_transform(sentence2.split()).toarray()\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "l = [print(vectorizer_fit[i], word) for i, word in enumerate(feature_names)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF  == terms frequency & inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "TF = \\frac{\\sum{specWords}}{\\sum{Words}};\n",
    "$$\n",
    "$$\n",
    "IDF = \\log{\\frac{\\sum{docs}}{\\sum{docsWithWord}}}\n",
    "$$\n",
    "Regarding to this:\n",
    "$$\n",
    "TF-IDF = TF * IDF == \\frac{\\sum{specWords}}{\\sum{Words}} \\cdot \\log{\\frac{\\sum{docs}}{\\sum{docsWithWord}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A thorough list of publicly available NLP data sets has already been created by Nicolas Iderhoff', ' Beyond these, here are some projects I can recommend to any NLP novice wanting to get their hands dirty:']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.26255634, 0.        , 0.        , 0.26255634, 0.26255634,\n",
       "         0.        , 0.26255634, 0.        , 0.26255634, 0.26255634,\n",
       "         0.        , 0.        , 0.        , 0.26255634, 0.        ,\n",
       "         0.26255634, 0.26255634, 0.26255634, 0.186811  , 0.        ,\n",
       "         0.26255634, 0.        , 0.26255634, 0.        , 0.26255634,\n",
       "         0.        , 0.        , 0.        , 0.26255634, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.22641916, 0.22641916, 0.        , 0.        ,\n",
       "         0.22641916, 0.        , 0.22641916, 0.        , 0.        ,\n",
       "         0.22641916, 0.22641916, 0.22641916, 0.        , 0.22641916,\n",
       "         0.        , 0.        , 0.        , 0.1610991 , 0.22641916,\n",
       "         0.        , 0.22641916, 0.        , 0.22641916, 0.        ,\n",
       "         0.22641916, 0.22641916, 0.22641916, 0.        , 0.45283832,\n",
       "         0.22641916]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'A thorough list of publicly available NLP data sets has already been created by Nicolas Iderhoff. Beyond these, here are some projects I can recommend to any NLP novice wanting to get their hands dirty:'\n",
    "text = text.split('.')\n",
    "print(text)\n",
    "vector = TfidfVectorizer()\n",
    "X = vector.fit_transform(text)\n",
    "X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "data = [\n",
    " ('I love my country.', 'positive'),\n",
    " ('This is an amazing place!', 'positive'),\n",
    " ('I do not like the smell of this place.', 'negative'),\n",
    " ('I do not like this restaurant', 'negative'),\n",
    " ('I am tired of hearing your nonsense.', 'negative'),\n",
    " (\"I always aspire to be like him\", 'positive'),\n",
    " (\"It's a horrible performance.\", \"negative\")\n",
    " ]\n",
    "model = NaiveBayesClassifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify('Dont watch it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('blood_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "      <th>whether he/she donated blood in March 2007</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2000</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>3750</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "460                21                  1                    250   \n",
       "309                16                  3                    750   \n",
       "499                74                  1                    250   \n",
       "144                 4                  2                    500   \n",
       "93                  4                  2                    500   \n",
       "340                14                  4                   1000   \n",
       "533                 2                  4                   1000   \n",
       "256                16                  8                   2000   \n",
       "17                  2                 15                   3750   \n",
       "608                 4                  1                    250   \n",
       "400                18                  2                    500   \n",
       "\n",
       "     Time (months)  whether he/she donated blood in March 2007  \n",
       "460             21                                           0  \n",
       "309             19                                           0  \n",
       "499             74                                           0  \n",
       "144              9                                           1  \n",
       "93               4                                           0  \n",
       "340             30                                           0  \n",
       "533             14                                           0  \n",
       "256             28                                           0  \n",
       "17              49                                           1  \n",
       "608              4                                           0  \n",
       "400             23                                           0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split features & true output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['whether he/she donated blood in March 2007']\n",
    "X = df.drop(columns='whether he/she donated blood in March 2007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1250</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2000</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)\n",
       "328                14                  4                   1000             28\n",
       "450                23                  3                    750             33\n",
       "167                 2                  1                    250              2\n",
       "377                14                  1                    250             14\n",
       "120                 2                  3                    750             16\n",
       "19                  2                  3                    750              4\n",
       "273                 4                  5                   1250             58\n",
       "61                  2                  8                   2000             35\n",
       "576                 3                  4                   1000             29\n",
       "47                  2                  2                    500              2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 748 entries, 0 to 747\n",
      "Data columns (total 4 columns):\n",
      "Recency (months)         748 non-null int64\n",
      "Frequency (times)        748 non-null int64\n",
      "Monetary (c.c. blood)    748 non-null int64\n",
      "Time (months)            748 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 23.5 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recency (months)</th>\n",
       "      <th>Frequency (times)</th>\n",
       "      <th>Monetary (c.c. blood)</th>\n",
       "      <th>Time (months)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "      <td>748.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.506684</td>\n",
       "      <td>5.514706</td>\n",
       "      <td>1378.676471</td>\n",
       "      <td>34.282086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.095396</td>\n",
       "      <td>5.839307</td>\n",
       "      <td>1459.826781</td>\n",
       "      <td>24.376714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1750.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>12500.000000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Recency (months)  Frequency (times)  Monetary (c.c. blood)  \\\n",
       "count        748.000000         748.000000             748.000000   \n",
       "mean           9.506684           5.514706            1378.676471   \n",
       "std            8.095396           5.839307            1459.826781   \n",
       "min            0.000000           1.000000             250.000000   \n",
       "25%            2.750000           2.000000             500.000000   \n",
       "50%            7.000000           4.000000            1000.000000   \n",
       "75%           14.000000           7.000000            1750.000000   \n",
       "max           74.000000          50.000000           12500.000000   \n",
       "\n",
       "       Time (months)  \n",
       "count     748.000000  \n",
       "mean       34.282086  \n",
       "std        24.376714  \n",
       "min         2.000000  \n",
       "25%        16.000000  \n",
       "50%        28.000000  \n",
       "75%        50.000000  \n",
       "max        98.000000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Design a multilayer perceptron with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(8, input_dim=4, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(8, input_dim=X.shape[1], init='uniform', activation='relu'))\n",
    "mlp.add(Dense(6, init='uniform', activation='relu'))\n",
    "# Last layer has sigmoid activation function to output values between 0 & 1\n",
    "mlp.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.compile(loss='binary_crossentropy', optimizer=adam_opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x7f8d22e997f0>,\n",
       " <keras.layers.core.Dense at 0x7f8d22e99a90>,\n",
       " <keras.layers.core.Dense at 0x7f8d22eafcc0>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "& Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "748/748 [==============================] - 0s 638us/step - loss: 0.5417 - acc: 0.7620\n",
      "Epoch 2/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.5040 - acc: 0.7620\n",
      "Epoch 3/200\n",
      "748/748 [==============================] - 0s 254us/step - loss: 0.5114 - acc: 0.7620\n",
      "Epoch 4/200\n",
      "748/748 [==============================] - 0s 247us/step - loss: 0.5133 - acc: 0.7620\n",
      "Epoch 5/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.5060 - acc: 0.7620\n",
      "Epoch 6/200\n",
      "748/748 [==============================] - 0s 232us/step - loss: 0.5067 - acc: 0.7620\n",
      "Epoch 7/200\n",
      "748/748 [==============================] - 0s 238us/step - loss: 0.5067 - acc: 0.7620\n",
      "Epoch 8/200\n",
      "748/748 [==============================] - 0s 254us/step - loss: 0.4920 - acc: 0.7620\n",
      "Epoch 9/200\n",
      "748/748 [==============================] - 0s 242us/step - loss: 0.5164 - acc: 0.7620\n",
      "Epoch 10/200\n",
      "748/748 [==============================] - 0s 244us/step - loss: 0.5056 - acc: 0.7620\n",
      "Epoch 11/200\n",
      "748/748 [==============================] - 0s 248us/step - loss: 0.5178 - acc: 0.7620\n",
      "Epoch 12/200\n",
      "748/748 [==============================] - 0s 231us/step - loss: 0.5033 - acc: 0.7620\n",
      "Epoch 13/200\n",
      "748/748 [==============================] - 0s 249us/step - loss: 0.5022 - acc: 0.7620\n",
      "Epoch 14/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.5120 - acc: 0.7620\n",
      "Epoch 15/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.5154 - acc: 0.7620\n",
      "Epoch 16/200\n",
      "748/748 [==============================] - 0s 268us/step - loss: 0.4987 - acc: 0.7620\n",
      "Epoch 17/200\n",
      "748/748 [==============================] - 0s 243us/step - loss: 0.5125 - acc: 0.7620\n",
      "Epoch 18/200\n",
      "748/748 [==============================] - 0s 257us/step - loss: 0.5184 - acc: 0.7620\n",
      "Epoch 19/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.5117 - acc: 0.7620\n",
      "Epoch 20/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.5043 - acc: 0.7620\n",
      "Epoch 21/200\n",
      "748/748 [==============================] - 0s 242us/step - loss: 0.5052 - acc: 0.7620\n",
      "Epoch 22/200\n",
      "748/748 [==============================] - 0s 243us/step - loss: 0.5001 - acc: 0.7620\n",
      "Epoch 23/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.5207 - acc: 0.7620\n",
      "Epoch 24/200\n",
      "748/748 [==============================] - 0s 269us/step - loss: 0.5029 - acc: 0.7620\n",
      "Epoch 25/200\n",
      "748/748 [==============================] - 0s 259us/step - loss: 0.5041 - acc: 0.7620\n",
      "Epoch 26/200\n",
      "748/748 [==============================] - 0s 256us/step - loss: 0.5016 - acc: 0.7620\n",
      "Epoch 27/200\n",
      "748/748 [==============================] - 0s 257us/step - loss: 0.5250 - acc: 0.7620\n",
      "Epoch 28/200\n",
      "748/748 [==============================] - 0s 296us/step - loss: 0.5175 - acc: 0.7620\n",
      "Epoch 29/200\n",
      "748/748 [==============================] - 0s 289us/step - loss: 0.5012 - acc: 0.7620\n",
      "Epoch 30/200\n",
      "748/748 [==============================] - 0s 258us/step - loss: 0.5035 - acc: 0.7620\n",
      "Epoch 31/200\n",
      "748/748 [==============================] - 0s 277us/step - loss: 0.5039 - acc: 0.7620\n",
      "Epoch 32/200\n",
      "748/748 [==============================] - 0s 265us/step - loss: 0.5119 - acc: 0.7620\n",
      "Epoch 33/200\n",
      "748/748 [==============================] - 0s 247us/step - loss: 0.5098 - acc: 0.7620\n",
      "Epoch 34/200\n",
      "748/748 [==============================] - 0s 250us/step - loss: 0.5076 - acc: 0.7620\n",
      "Epoch 35/200\n",
      "748/748 [==============================] - 0s 258us/step - loss: 0.4998 - acc: 0.7620\n",
      "Epoch 36/200\n",
      "748/748 [==============================] - 0s 255us/step - loss: 0.5005 - acc: 0.7620\n",
      "Epoch 37/200\n",
      "748/748 [==============================] - 0s 256us/step - loss: 0.5136 - acc: 0.7620\n",
      "Epoch 38/200\n",
      "748/748 [==============================] - 0s 246us/step - loss: 0.5036 - acc: 0.7620\n",
      "Epoch 39/200\n",
      "748/748 [==============================] - 0s 264us/step - loss: 0.4977 - acc: 0.7620\n",
      "Epoch 40/200\n",
      "748/748 [==============================] - 0s 295us/step - loss: 0.5005 - acc: 0.7620\n",
      "Epoch 41/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.4998 - acc: 0.7620\n",
      "Epoch 42/200\n",
      "748/748 [==============================] - 0s 249us/step - loss: 0.5084 - acc: 0.7620\n",
      "Epoch 43/200\n",
      "748/748 [==============================] - 0s 270us/step - loss: 0.5154 - acc: 0.7620\n",
      "Epoch 44/200\n",
      "748/748 [==============================] - 0s 261us/step - loss: 0.5076 - acc: 0.7620\n",
      "Epoch 45/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.4981 - acc: 0.7620\n",
      "Epoch 46/200\n",
      "748/748 [==============================] - 0s 255us/step - loss: 0.4942 - acc: 0.7620\n",
      "Epoch 47/200\n",
      "748/748 [==============================] - 0s 272us/step - loss: 0.5036 - acc: 0.7620\n",
      "Epoch 48/200\n",
      "748/748 [==============================] - 0s 254us/step - loss: 0.5030 - acc: 0.7620\n",
      "Epoch 49/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.5010 - acc: 0.7620\n",
      "Epoch 50/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.5078 - acc: 0.7620\n",
      "Epoch 51/200\n",
      "748/748 [==============================] - 0s 251us/step - loss: 0.5035 - acc: 0.7620\n",
      "Epoch 52/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.5007 - acc: 0.7620\n",
      "Epoch 53/200\n",
      "748/748 [==============================] - 0s 263us/step - loss: 0.5012 - acc: 0.7620\n",
      "Epoch 54/200\n",
      "748/748 [==============================] - 0s 280us/step - loss: 0.4989 - acc: 0.7620\n",
      "Epoch 55/200\n",
      "748/748 [==============================] - 0s 278us/step - loss: 0.4911 - acc: 0.7620\n",
      "Epoch 56/200\n",
      "748/748 [==============================] - 0s 274us/step - loss: 0.4923 - acc: 0.7620\n",
      "Epoch 57/200\n",
      "748/748 [==============================] - 0s 285us/step - loss: 0.5009 - acc: 0.7620\n",
      "Epoch 58/200\n",
      "748/748 [==============================] - 0s 320us/step - loss: 0.4942 - acc: 0.7620\n",
      "Epoch 59/200\n",
      "748/748 [==============================] - 0s 264us/step - loss: 0.4987 - acc: 0.7620\n",
      "Epoch 60/200\n",
      "748/748 [==============================] - 0s 260us/step - loss: 0.4929 - acc: 0.7620\n",
      "Epoch 61/200\n",
      "748/748 [==============================] - 0s 257us/step - loss: 0.5144 - acc: 0.7620\n",
      "Epoch 62/200\n",
      "748/748 [==============================] - 0s 265us/step - loss: 0.5008 - acc: 0.7620\n",
      "Epoch 63/200\n",
      "748/748 [==============================] - 0s 254us/step - loss: 0.4997 - acc: 0.7620\n",
      "Epoch 64/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.4919 - acc: 0.7620\n",
      "Epoch 65/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.5127 - acc: 0.7620\n",
      "Epoch 66/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.5106 - acc: 0.7620\n",
      "Epoch 67/200\n",
      "748/748 [==============================] - 0s 248us/step - loss: 0.4973 - acc: 0.7620\n",
      "Epoch 68/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.4889 - acc: 0.7620\n",
      "Epoch 69/200\n",
      "748/748 [==============================] - 0s 250us/step - loss: 0.5183 - acc: 0.7620\n",
      "Epoch 70/200\n",
      "748/748 [==============================] - 0s 248us/step - loss: 0.5127 - acc: 0.7620\n",
      "Epoch 71/200\n",
      "748/748 [==============================] - 0s 271us/step - loss: 0.5019 - acc: 0.7620\n",
      "Epoch 72/200\n",
      "748/748 [==============================] - 0s 261us/step - loss: 0.5027 - acc: 0.7620\n",
      "Epoch 73/200\n",
      "748/748 [==============================] - 0s 260us/step - loss: 0.5113 - acc: 0.7620\n",
      "Epoch 74/200\n",
      "748/748 [==============================] - 0s 268us/step - loss: 0.5056 - acc: 0.7620\n",
      "Epoch 75/200\n",
      "748/748 [==============================] - 0s 269us/step - loss: 0.5037 - acc: 0.7620\n",
      "Epoch 76/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.5056 - acc: 0.7620\n",
      "Epoch 77/200\n",
      "748/748 [==============================] - 0s 262us/step - loss: 0.4991 - acc: 0.7620\n",
      "Epoch 78/200\n",
      "748/748 [==============================] - 0s 272us/step - loss: 0.5014 - acc: 0.7620\n",
      "Epoch 79/200\n",
      "748/748 [==============================] - 0s 258us/step - loss: 0.5026 - acc: 0.7620\n",
      "Epoch 80/200\n",
      "748/748 [==============================] - 0s 270us/step - loss: 0.5076 - acc: 0.7620\n",
      "Epoch 81/200\n",
      "748/748 [==============================] - 0s 251us/step - loss: 0.5121 - acc: 0.7620\n",
      "Epoch 82/200\n",
      "748/748 [==============================] - 0s 259us/step - loss: 0.4971 - acc: 0.7620\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 0s 251us/step - loss: 0.5074 - acc: 0.7620\n",
      "Epoch 84/200\n",
      "748/748 [==============================] - 0s 226us/step - loss: 0.5074 - acc: 0.7620\n",
      "Epoch 85/200\n",
      "748/748 [==============================] - 0s 235us/step - loss: 0.5024 - acc: 0.7620\n",
      "Epoch 86/200\n",
      "748/748 [==============================] - 0s 249us/step - loss: 0.5041 - acc: 0.7620\n",
      "Epoch 87/200\n",
      "748/748 [==============================] - 0s 244us/step - loss: 0.4965 - acc: 0.7620\n",
      "Epoch 88/200\n",
      "748/748 [==============================] - 0s 236us/step - loss: 0.5069 - acc: 0.7620\n",
      "Epoch 89/200\n",
      "748/748 [==============================] - 0s 237us/step - loss: 0.4960 - acc: 0.7620\n",
      "Epoch 90/200\n",
      "748/748 [==============================] - 0s 230us/step - loss: 0.5190 - acc: 0.7620\n",
      "Epoch 91/200\n",
      "748/748 [==============================] - 0s 233us/step - loss: 0.5074 - acc: 0.7620\n",
      "Epoch 92/200\n",
      "748/748 [==============================] - 0s 234us/step - loss: 0.5009 - acc: 0.7620\n",
      "Epoch 93/200\n",
      "748/748 [==============================] - 0s 233us/step - loss: 0.4991 - acc: 0.7620\n",
      "Epoch 94/200\n",
      "748/748 [==============================] - 0s 228us/step - loss: 0.4997 - acc: 0.7620\n",
      "Epoch 95/200\n",
      "748/748 [==============================] - 0s 257us/step - loss: 0.5091 - acc: 0.7620\n",
      "Epoch 96/200\n",
      "748/748 [==============================] - 0s 227us/step - loss: 0.5001 - acc: 0.7620\n",
      "Epoch 97/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.5039 - acc: 0.7620\n",
      "Epoch 98/200\n",
      "748/748 [==============================] - 0s 242us/step - loss: 0.5101 - acc: 0.7620\n",
      "Epoch 99/200\n",
      "748/748 [==============================] - 0s 245us/step - loss: 0.4996 - acc: 0.7620\n",
      "Epoch 100/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.5002 - acc: 0.7620\n",
      "Epoch 101/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.4979 - acc: 0.7620\n",
      "Epoch 102/200\n",
      "748/748 [==============================] - 0s 233us/step - loss: 0.4965 - acc: 0.7620\n",
      "Epoch 103/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.4999 - acc: 0.7620\n",
      "Epoch 104/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.4945 - acc: 0.7620\n",
      "Epoch 105/200\n",
      "748/748 [==============================] - 0s 244us/step - loss: 0.4929 - acc: 0.7620\n",
      "Epoch 106/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.4908 - acc: 0.7620\n",
      "Epoch 107/200\n",
      "748/748 [==============================] - 0s 331us/step - loss: 0.4924 - acc: 0.7620\n",
      "Epoch 108/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.4999 - acc: 0.7620\n",
      "Epoch 109/200\n",
      "748/748 [==============================] - 0s 255us/step - loss: 0.4995 - acc: 0.7620\n",
      "Epoch 110/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.4991 - acc: 0.7620\n",
      "Epoch 111/200\n",
      "748/748 [==============================] - 0s 250us/step - loss: 0.4871 - acc: 0.7620\n",
      "Epoch 112/200\n",
      "748/748 [==============================] - 0s 245us/step - loss: 0.4899 - acc: 0.7620\n",
      "Epoch 113/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.5042 - acc: 0.7620\n",
      "Epoch 114/200\n",
      "748/748 [==============================] - 0s 275us/step - loss: 0.4876 - acc: 0.7620\n",
      "Epoch 115/200\n",
      "748/748 [==============================] - 0s 259us/step - loss: 0.5296 - acc: 0.7620\n",
      "Epoch 116/200\n",
      "748/748 [==============================] - 0s 238us/step - loss: 0.4949 - acc: 0.7620\n",
      "Epoch 117/200\n",
      "748/748 [==============================] - 0s 245us/step - loss: 0.4926 - acc: 0.7620\n",
      "Epoch 118/200\n",
      "748/748 [==============================] - 0s 297us/step - loss: 0.4867 - acc: 0.7620\n",
      "Epoch 119/200\n",
      "748/748 [==============================] - 0s 264us/step - loss: 0.4946 - acc: 0.7620\n",
      "Epoch 120/200\n",
      "748/748 [==============================] - 0s 257us/step - loss: 0.4958 - acc: 0.7620\n",
      "Epoch 121/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.5055 - acc: 0.7620\n",
      "Epoch 122/200\n",
      "748/748 [==============================] - 0s 249us/step - loss: 0.4989 - acc: 0.7620\n",
      "Epoch 123/200\n",
      "748/748 [==============================] - 0s 259us/step - loss: 0.5089 - acc: 0.7620\n",
      "Epoch 124/200\n",
      "748/748 [==============================] - 0s 268us/step - loss: 0.4955 - acc: 0.7620\n",
      "Epoch 125/200\n",
      "748/748 [==============================] - 0s 265us/step - loss: 0.5068 - acc: 0.7620\n",
      "Epoch 126/200\n",
      "748/748 [==============================] - 0s 266us/step - loss: 0.4963 - acc: 0.7620\n",
      "Epoch 127/200\n",
      "748/748 [==============================] - 0s 297us/step - loss: 0.5042 - acc: 0.7620\n",
      "Epoch 128/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.5088 - acc: 0.7620\n",
      "Epoch 129/200\n",
      "748/748 [==============================] - 0s 257us/step - loss: 0.4922 - acc: 0.7620\n",
      "Epoch 130/200\n",
      "748/748 [==============================] - 0s 282us/step - loss: 0.5161 - acc: 0.7620\n",
      "Epoch 131/200\n",
      "748/748 [==============================] - 0s 262us/step - loss: 0.4984 - acc: 0.7620\n",
      "Epoch 132/200\n",
      "748/748 [==============================] - 0s 235us/step - loss: 0.5091 - acc: 0.7620\n",
      "Epoch 133/200\n",
      "748/748 [==============================] - 0s 249us/step - loss: 0.5212 - acc: 0.7620\n",
      "Epoch 134/200\n",
      "748/748 [==============================] - 0s 309us/step - loss: 0.4982 - acc: 0.7620\n",
      "Epoch 135/200\n",
      "748/748 [==============================] - 0s 302us/step - loss: 0.4985 - acc: 0.7620\n",
      "Epoch 136/200\n",
      "748/748 [==============================] - 0s 328us/step - loss: 0.4929 - acc: 0.7620\n",
      "Epoch 137/200\n",
      "748/748 [==============================] - 0s 285us/step - loss: 0.4917 - acc: 0.7620\n",
      "Epoch 138/200\n",
      "748/748 [==============================] - 0s 306us/step - loss: 0.5014 - acc: 0.7620\n",
      "Epoch 139/200\n",
      "748/748 [==============================] - 0s 258us/step - loss: 0.4899 - acc: 0.7620\n",
      "Epoch 140/200\n",
      "748/748 [==============================] - 0s 269us/step - loss: 0.5000 - acc: 0.7620\n",
      "Epoch 141/200\n",
      "748/748 [==============================] - 0s 279us/step - loss: 0.5010 - acc: 0.7620\n",
      "Epoch 142/200\n",
      "748/748 [==============================] - 0s 247us/step - loss: 0.4955 - acc: 0.7620\n",
      "Epoch 143/200\n",
      "748/748 [==============================] - 0s 264us/step - loss: 0.5091 - acc: 0.7620\n",
      "Epoch 144/200\n",
      "748/748 [==============================] - 0s 264us/step - loss: 0.5019 - acc: 0.7620\n",
      "Epoch 145/200\n",
      "748/748 [==============================] - 0s 274us/step - loss: 0.5061 - acc: 0.7620\n",
      "Epoch 146/200\n",
      "748/748 [==============================] - 0s 272us/step - loss: 0.4895 - acc: 0.7620\n",
      "Epoch 147/200\n",
      "748/748 [==============================] - 0s 268us/step - loss: 0.4921 - acc: 0.7620\n",
      "Epoch 148/200\n",
      "748/748 [==============================] - 0s 285us/step - loss: 0.4890 - acc: 0.7620\n",
      "Epoch 149/200\n",
      "748/748 [==============================] - 0s 267us/step - loss: 0.4957 - acc: 0.7620\n",
      "Epoch 150/200\n",
      "748/748 [==============================] - 0s 254us/step - loss: 0.4940 - acc: 0.7620\n",
      "Epoch 151/200\n",
      "748/748 [==============================] - 0s 248us/step - loss: 0.4914 - acc: 0.7620\n",
      "Epoch 152/200\n",
      "748/748 [==============================] - 0s 252us/step - loss: 0.4931 - acc: 0.7620\n",
      "Epoch 153/200\n",
      "748/748 [==============================] - 0s 276us/step - loss: 0.4955 - acc: 0.7620\n",
      "Epoch 154/200\n",
      "748/748 [==============================] - 0s 290us/step - loss: 0.5062 - acc: 0.7620\n",
      "Epoch 155/200\n",
      "748/748 [==============================] - 0s 266us/step - loss: 0.4990 - acc: 0.7620\n",
      "Epoch 156/200\n",
      "748/748 [==============================] - 0s 260us/step - loss: 0.5046 - acc: 0.7620\n",
      "Epoch 157/200\n",
      "748/748 [==============================] - 0s 275us/step - loss: 0.4881 - acc: 0.7620\n",
      "Epoch 158/200\n",
      "748/748 [==============================] - 0s 263us/step - loss: 0.4934 - acc: 0.7620\n",
      "Epoch 159/200\n",
      "748/748 [==============================] - 0s 265us/step - loss: 0.4884 - acc: 0.7620\n",
      "Epoch 160/200\n",
      "748/748 [==============================] - 0s 261us/step - loss: 0.4912 - acc: 0.7620\n",
      "Epoch 161/200\n",
      "748/748 [==============================] - 0s 254us/step - loss: 0.5002 - acc: 0.7620\n",
      "Epoch 162/200\n",
      "748/748 [==============================] - 0s 259us/step - loss: 0.4896 - acc: 0.7620\n",
      "Epoch 163/200\n",
      "748/748 [==============================] - 0s 253us/step - loss: 0.5166 - acc: 0.7620\n",
      "Epoch 164/200\n",
      "748/748 [==============================] - 0s 268us/step - loss: 0.4982 - acc: 0.7620\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 0s 242us/step - loss: 0.4960 - acc: 0.7620\n",
      "Epoch 166/200\n",
      "748/748 [==============================] - 0s 238us/step - loss: 0.4941 - acc: 0.7620\n",
      "Epoch 167/200\n",
      "748/748 [==============================] - 0s 228us/step - loss: 0.5217 - acc: 0.7620\n",
      "Epoch 168/200\n",
      "748/748 [==============================] - 0s 234us/step - loss: 0.5006 - acc: 0.7620\n",
      "Epoch 169/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.4941 - acc: 0.7620\n",
      "Epoch 170/200\n",
      "748/748 [==============================] - 0s 235us/step - loss: 0.5066 - acc: 0.7620\n",
      "Epoch 171/200\n",
      "748/748 [==============================] - 0s 231us/step - loss: 0.4933 - acc: 0.7620\n",
      "Epoch 172/200\n",
      "748/748 [==============================] - 0s 235us/step - loss: 0.5045 - acc: 0.7620\n",
      "Epoch 173/200\n",
      "748/748 [==============================] - 0s 238us/step - loss: 0.4946 - acc: 0.7620\n",
      "Epoch 174/200\n",
      "748/748 [==============================] - 0s 235us/step - loss: 0.5063 - acc: 0.7620\n",
      "Epoch 175/200\n",
      "748/748 [==============================] - 0s 237us/step - loss: 0.4961 - acc: 0.7620\n",
      "Epoch 176/200\n",
      "748/748 [==============================] - 0s 250us/step - loss: 0.4919 - acc: 0.7620\n",
      "Epoch 177/200\n",
      "748/748 [==============================] - 0s 242us/step - loss: 0.4920 - acc: 0.7620\n",
      "Epoch 178/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.4956 - acc: 0.7620\n",
      "Epoch 179/200\n",
      "748/748 [==============================] - 0s 235us/step - loss: 0.4975 - acc: 0.7620\n",
      "Epoch 180/200\n",
      "748/748 [==============================] - 0s 230us/step - loss: 0.4963 - acc: 0.7620\n",
      "Epoch 181/200\n",
      "748/748 [==============================] - 0s 256us/step - loss: 0.4914 - acc: 0.7620\n",
      "Epoch 182/200\n",
      "748/748 [==============================] - 0s 225us/step - loss: 0.4979 - acc: 0.7620\n",
      "Epoch 183/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.4889 - acc: 0.7620\n",
      "Epoch 184/200\n",
      "748/748 [==============================] - 0s 246us/step - loss: 0.5026 - acc: 0.7620\n",
      "Epoch 185/200\n",
      "748/748 [==============================] - 0s 229us/step - loss: 0.4895 - acc: 0.7620\n",
      "Epoch 186/200\n",
      "748/748 [==============================] - 0s 242us/step - loss: 0.4901 - acc: 0.7620\n",
      "Epoch 187/200\n",
      "748/748 [==============================] - 0s 240us/step - loss: 0.4993 - acc: 0.7620\n",
      "Epoch 188/200\n",
      "748/748 [==============================] - 0s 246us/step - loss: 0.4949 - acc: 0.7620\n",
      "Epoch 189/200\n",
      "748/748 [==============================] - 0s 238us/step - loss: 0.4911 - acc: 0.7620\n",
      "Epoch 190/200\n",
      "748/748 [==============================] - 0s 249us/step - loss: 0.4999 - acc: 0.7620\n",
      "Epoch 191/200\n",
      "748/748 [==============================] - 0s 234us/step - loss: 0.4920 - acc: 0.7620\n",
      "Epoch 192/200\n",
      "748/748 [==============================] - 0s 242us/step - loss: 0.4968 - acc: 0.7620\n",
      "Epoch 193/200\n",
      "748/748 [==============================] - 0s 245us/step - loss: 0.4932 - acc: 0.7620\n",
      "Epoch 194/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.4936 - acc: 0.7620\n",
      "Epoch 195/200\n",
      "748/748 [==============================] - 0s 234us/step - loss: 0.5027 - acc: 0.7620\n",
      "Epoch 196/200\n",
      "748/748 [==============================] - 0s 234us/step - loss: 0.4983 - acc: 0.7620\n",
      "Epoch 197/200\n",
      "748/748 [==============================] - 0s 239us/step - loss: 0.4913 - acc: 0.7620\n",
      "Epoch 198/200\n",
      "748/748 [==============================] - 0s 259us/step - loss: 0.4963 - acc: 0.7620\n",
      "Epoch 199/200\n",
      "748/748 [==============================] - 0s 241us/step - loss: 0.4868 - acc: 0.7620\n",
      "Epoch 200/200\n",
      "748/748 [==============================] - 0s 238us/step - loss: 0.5060 - acc: 0.7620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cc13fe748>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X, y, nb_epoch=200, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748/748 [==============================] - 0s 86us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4786351006278858, 0.7620320855614974]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yurii/jupyter_envs/nlp/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22994652406417113"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(clf.predict(X) != y).sum() / X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
